{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 4.1 Landmark-based models\n\nIn order to carry out tasks like localization or navigation, a mobile robot has to perceive its workspace. A variety of sensors can be used for that, as well as a number of probabilistic models for managing their behavior."},{"metadata":{},"cell_type":"markdown","source":"Typically, the sensors used onboard the robot do not deliver the exact truth of the quantities they are measuring, but a perturbed version. This is due to the working (physical) principles that govern the sensors behavior, and to the conditions of their workspaces (illumination, humidity, temperature, etc.). \n\nAs an illustrative example of this, there is a popular European company called [Sick](https://www.sick.com/es/es/), which develops 2D LiDAR sensors (among other devices). One of its most popular sensors is the [TiM2xx one](https://www.sick.com/gb/en/detection-and-ranging-solutions/2d-lidar-sensors/tim2xx/tim240-2050300/p/p654443) (see left part of Fig.1), which can be easily integrable into a robotic platform. If we take a look at the specifications about the performance of such device, we can check how this uncertainty about the sensor measurements is explicitly specified (systematic error and statistical error), as well as how these values depend on environmental conditions (see right part of Fig.1). $\\\\[10pt]$\n\n<figure style=\"text-align:center\">\n  <img src=\"images/sick-laser.PNG\" alt=\"\">\n  <figcaption>Fig. 1: Left, TiM2xx sensor from Sick. Right, performance details of such sensor.</figcaption>\n</figure>    \n\n$\\\\[5pt]$To account for this behavior, sensors' measurements in probabilistic robotics will be modeled by... wait for it... the probability distribution $p(z|v)$, where z models the measurement and v is the ground truth."},{"metadata":{},"cell_type":"markdown","source":"## 4.1.1 Dealing with landmark-based models\n\nIn different applications it is interesting for the robot to detect landmarks in its workspace and build internal representations of them, commonly referred to as maps. In the case of maps consisting of a collection of landmarks $m=\\{m_i\\}, i=1,\\dots,N$, different types of sensors can be used to provide observations $z_i$ of those landmarks:\n\n- **Distance/range** (*e.g.* radio, GPS, etc.): $\\\\[5pt]$<br />\n  $\\hspace{1cm}$ $z_i = d_i = h_i(x,m)+w_i$ $\\\\[5pt]$\n- **Bearing** (*e.g.* camera): $\\\\[5pt]$<br />\n  $\\hspace{1cm}$$z_i = \\theta_i = h_i(x,m)+w_i$ $\\\\[5pt]$\n- **Distance/range and bearing** (*e.g.* stereo, features in a scan, etc.) $\\\\[5pt]$<br />\n  $\\hspace{1cm}$$z_i = [d_i,\\theta_i]^T = h_i(x,m)+w_i$ *(in this case, $h_i(x,m)$ and $w_i$ are 2D vectors)* $\\\\[5pt]$\n\nwhere:\n\n- $z_i$ is an observation, $x$ is the sensor pose, and $m$ is the map of the environment,\n- $h(x,m)$ is the Observation (or measurement, or prediction) function: it predicts the value of the observation $z_i$ given the state values $x$ and $m$, and\n- $w$ is an error, modeled by a gaussian distribution as $w=[h(x,m)-z_i]\\sim N(O,Q)$, being $Q$ the uncertainty in the observation error. \n\nIn this way, the probability distribution $p(z|x,m)$ modeling the sensor measurements results:\n\n$$\np(z|x,m) = K \\exp\\{-\\frac{1}{2}[h(x,m)-z]^T Q^{-1} [h(x,m)-z]\\}\n$$\n\nThese types of maps and sensor measurements pose a new problem: **data association**, that is, with which landmark $m_i$ correspond the observation $z_i$ to: \n\n$$h_i(x,m)=h(x,m_i)$$\n\nThis problem is usually addressed by applying Chi-squared tests, although for the shake of simplicity in this book we will consider it as solved. "},{"metadata":{},"cell_type":"markdown","source":"### Playing with landmarks and robot poses\n\nIn the remaining of this section we will familiarize ourselves with the process of observing landmarks from robots located at certain poses, as well as the transformations needed to make use of these observations, that is, to express those observations into the world frame and backwards.\n\nSome relevant concepts:\n\n- **World frame**: $(x, y)$ coordinates from a selected point of reference $(0, 0)$. We use to keep track of the robots pose, and within the map.\n- **Observation**: Information from the real world provided by a sensor, from the point of view (*pov*) of a certain robot.\n- **Range-bearing sensor**: Sensor model being used in this lesson. This kind of sensors detect how far is an object $(d)$ and its orientation relative to the robot's one $(\\theta)$.\n\nThe main tools to deal with those concepts are:\n\n- the composition of two poses.\n- the composition of a pose and a landmark.\n- the propagation of uncertainty through the Jacobians of these compositions."},{"metadata":{},"cell_type":"markdown","source":"We will address several problems of incremental complexity. In all of them, it is important to have in mind how the composition of a (robot) pose and a landmark point works:\n\n\n<center>\n    <a id=\"figure\"></a>\n    <img src=\"images/fig4-1.PNG\">\n    <figcaption>Fig. 1: Composition of a pose and a landmark point.</figcaption>\n</center>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#%matplotlib notebook\n%matplotlib inline\n\n# IMPORTS\n\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\nimport sys\nsys.path.append(\"..\")\nfrom utils.PlotEllipse import PlotEllipse\nfrom utils.DrawRobot import DrawRobot\nfrom utils.tcomp import tcomp\nfrom utils.tinv import tinv, jac_tinv1 as jac_tinv\nfrom utils.Jacobians import J1, J2","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **<span style=\"color:green\"><b><i>ASSIGNMENT 1: Expressing an observed landmark in coordinates of the world frame</i></b></span>** <a id=\"4211\"></a> \n\nLet’s consider a robot R1 at a perfectly known pose $p_1 = [1, 2, 0.5]^T$ (no uncertainty at this point) which observes a landmark $m$ with a range-bearing (polar) sensor affected by a zero-mean Gaussian error with covariance $W_{1p} = diag\\left([0.25, 0.04]\\right)$. The sensor provides the measurement $z_{1p} = [4m., 0.7rad.]^T$. The scenario is the one in Fig. 2.\n\n<center>\n    <img src=\"./images/assignment_1.PNG\"/>\n    <figcaption>Fig 2. Illustration of the scenario in assignment 1.</figcaption>\n</center>\n \n**You are tasked to** compute the Gaussian probability distribution  (mean and covariance) of the landmark observation in the world frame (the same as the robot)  and plot its corresponding ellipse (in magenta, $\\sigma=1$). Concretely, you have to complete the ``to_world_frame()`` function, and modify the demo code to show the ellipse representing the uncertainty. \n \n Consider the following:\n \n - You can express a sensor measurement in polar coordinates ($z_p=[r,\\alpha]^T$) as cartesian coordinates ($z_c=[z_x,z_y]^T)$ by:\n \n $$\n     z_c = \\begin{bmatrix} z_x \\\\ z_y \\end{bmatrix} \n         = \\begin{bmatrix} r \\ cos\\alpha \\\\ r \\ sin\\alpha \\end{bmatrix} \n         = f(r,\\alpha)\n $$ $\\\\[5pt]$\n \n - While computing the covariance of the landmark observation, you have to start by computing the covariance of the observation in the Cartesian robot $R1$ frame. That is:\n \n $$  \n W_{c} = \\frac{\\partial f(z_x,z_y)}{\\partial \\{r,\\alpha\\}} \\ W_{p} \\ \\frac{\\partial f(z_x,z_y)}{\\partial \\{r,\\alpha\\}}^T\n $$\n   \n   Then you can get the convariance in the world frame as:\n   \n   $$ W_{z\\_w} = \\frac{\\partial f(p,z_c)}{\\partial p} \\ Q_{p1\\_w} \\ \\left( \\frac{\\partial f(p,z_c)}{\\partial p} \\right)^T +\n        \\frac{\\partial f(p,z_c)}{\\partial z_c} \\ W_{c} \\ \\left( \\frac{\\partial f(p,z_c)}{\\partial z_c} \\right)^T\n   $$\n   \n   where $f(p,z_c) = p \\oplus z_c$, that is, the composition of the pose and the landmark.\n   \n   - Note that $\\frac{\\partial f(p,z_c)}{\\partial p}$ and $\\frac{\\partial f(p,z_c)}{\\partial z_c}$ are the same Jacobians as previously used to compose two poses in *robot motion*, but with a reduced size since **while working with landmarks the orientation is meaningless, only the position matters**. The functions ``J1()`` and ``J2()`` implement these jacobians for you. $\\\\[5pt]$\n   \nExample: $\\\\[5pt]$\n\n<center>\n    <img src=\"./images/result_1.PNG\" />\n    <figcaption>Fig 3. Pose of a robot (without uncertainty) and position of an observed landmark with its associated uncertainty.</figcaption>\n</center>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def to_world_frame(p1_w, Qp1_w, z1_p_r, W1):\n    \"\"\" Covert the observation z1_p_r to the world frame\n    \n        Args:\n            p1_w: Pose of the robot(in world frame)\n            Qp1_w: Covariance of the robot\n            z1_p_r: Observation to a landmark (polar coordinates) from robots perspective\n            W1: Covariance of the sensor in polar coordinates\n    \n        Returns:\n            z1_w: Pose of landmark in the world frame\n            Wz1: Covariance associated to z1_w\n    \"\"\"\n    \n    # Definition of useful variables\n    r, a = z1_p_r[0,0], z1_p_r[1,0]\n    s, c = np.sin(a), np.cos(a)\n\n    # Jacobian to convert the measurement uncertainty from polar to cartesian coordinates\n    Jac_pol_car = np.array([\n        [c, -r*s],\n        [s, r*c],\n    ])\n\n    # Built a tuple with:\n    # z1_car_rel[0]: coordinates of the sensor measurement in cartesian coordinates relative to robot position\n    # z1_car_rel[1]: its associated uncertainty expressed in cartesian coordinates\n    z1_car_rel = (\n            np.vstack([r*c, r*s, a]), # position\n            Jac_pol_car@W1@np.transpose(Jac_pol_car) # uncertainty\n            )\n    \n    z1_ext = np.vstack([z1_car_rel[0], 0]) # Extends z1 for its usage in the Jacobian functions J1 and J2\n\n    # Build the jacobians \n    Jac_ap = J1(p1_w ,z1_ext)[0:2,:] # Jacobian for expressing the uncertainty in the robot pose in a global frame\n    Jac_aa = J2(p1_w ,z1_ext)[0:2,0:2] # This one expresses the uncertainty in the measurment in a global frame\n    \n    z1_w = tcomp(p1_w, z1_car_rel[0])[0:2,[0]] # Compute coordinates of the landmark in the world\n    Wz1 = (Jac_ap @ Qp1_w @ np.transpose(Jac_ap)\n          + Jac_aa @ z1_car_rel[1] @ np.transpose(Jac_aa)) # Finally, propagate the covariance!\n    \n    return z1_w, Wz1","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Robot\np1_w = np.vstack([1, 2, 0.5]) # Robot R1 pose\nQp1_w = np.zeros((3, 3)) # Robot pose convariance matrix (uncertainty)\n\n# Landmark observation\nz1_p_r = np.vstack([4., .7]) # Measurement/Observation\nW1 = np.diag([0.25, 0.04]) # Sensor noise covariance\n\n# Express the landmark observation in the world frame (mean and covariance)\nz1_w, Wz1 = to_world_frame(p1_w, Qp1_w, z1_p_r, W1)\n\n# Visualize the results\nfig, ax = plt.subplots()\nplt.xlim([-.5, 10])\nplt.ylim([-.5, 10])\nplt.grid()\nplt.tight_layout()\n\nDrawRobot(fig, ax, p1_w, label='R1', color='blue')\n    \nax.plot(z1_w[0, 0], z1_w[1, 0], 'o', label='Z1', color='green')\nPlotEllipse(fig, ax, z1_w, Wz1, color='green')\n\nplt.legend()\nprint('----\\tExercise 4.1.1\\t----\\n'+\n      'z1_w = {}\\'\\n'.format(z1_w.flatten())\n      + 'Wz1_w = \\n{}\\n'.format(Wz1))","execution_count":16,"outputs":[{"output_type":"stream","text":"----\tExercise 4.1.1\t----\nz1_w = [2.44943102 5.72815634]'\nWz1_w = \n[[ 0.58879177 -0.13171532]\n [-0.13171532  0.30120823]]\n\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"<span style=\"color:blue\">Expected results for demo:</span>\n\n\n```\n---- Exercise 4.1.1 ----\nz1_w = [2.44943102 5.72815634]'\nWz1_w = \n[[ 0.58879177 -0.13171532]\n [-0.13171532  0.30120823]]\n```"},{"metadata":{},"cell_type":"markdown","source":"### **<span style=\"color:green\"><b><i>ASSIGNMENT 2: Adding uncertainty to the robot position</i></b></span>**\n\nNow, let’s assume that the robot pose is not known, but it is a RV that follows a\nGaussian probability distribution: $p_1 \\sim N([1, 2, 0.5]^T, \\Sigma_1)$ with $\\Sigma_1 = diag([0.08,0.6,\n0.02 ])$.\n\n1. Compute the covariance matrix $\\Sigma_{m1}$ of the landmark in the world frame and plot it as an ellipse centered at the mean $m_1$ (in blue, $sigma= 1$). Plot also the covariance of the robot pose (in blue, $sigma= 1$).\n     \n2. Compare the covariance with that obtained in the previous case.  $\\\\[5pt]$\n   \nExample: $\\\\[5pt]$\n\n<center>\n    <img src=\"./images/result_2.png\" />\n    <figcaption>Fig 4. Pose of a robot and position of an observed landmark, along with their associated uncertainties.</figcaption>\n</center>"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Robot\np1_w = np.vstack([1, 2, 0.5]) # Robot R1 pose\nQp1_w = np.diag([0.08,0.6,0.02])  # Robot pose convariance matrix (uncertainty)\n\n# Landmark observation\nz1_p_r = np.vstack([4., .7]) # Measurement/Observation\nW1 = np.diag([0.25, 0.04]) # Sensor noise covariance\n\n# Express the landmark observation in the world frame (mean and covariance)\nz1_w, Wz1 = to_world_frame(p1_w, Qp1_w, z1_p_r, W1)\n\n# MATPLOTLIB\nfig, ax = plt.subplots()\nplt.xlim([-.5, 10])\nplt.ylim([-.5, 10])\nplt.grid()\nplt.tight_layout()\n\nfig.canvas.draw()\n\nDrawRobot(fig, ax, p1_w, label='R1', color='red')  \nPlotEllipse(fig, ax, None, None, color='blue')\n\nax.plot(z1_w[0, 0], z1_w[1, 0], 'o', label='Z1', color='green')\nPlotEllipse(fig, ax, None, None, color='green')\n\nplt.legend()\nprint('---- Exercise 4.1.2 ----\\n'+\n      'Wz1_w = \\n{}\\n'.format(Wz1))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<span style=\"color:blue\">Expected results for demo:</span>\n\n```\n---- Exercise 4.1.2 ----\nWz1_w = \n[[ 0.94677477 -0.23978943]\n [-0.23978943  0.94322523]]\n```"},{"metadata":{},"cell_type":"markdown","source":"### **<span style=\"color:green\"><b><i>ASSIGNMENT 3: Getting the relative pose between two robots</i></b></span>** \n\nAnother robot `R2` is at pose $p_2 \\sim ([6m., 4m., 2.1rad.]^T, \\Sigma_2)$ with $\\Sigma_2 = diag([0.20,0.09,\n 0.03])$. Plot `p2` and its ellipse (covariance) in green ($sigma=1$). **Compute the relative pose `p12` between `R1` and `R2`**, including its associated uncertainty. This scenario is shown in Fig. 5.\n \n<center>\n    <img src=\"./images/assignment_3.png\"/>\n    <figcaption>Fig 5. Illustration of the scenario in this assignment.</figcaption>\n</center>\n \n This relative pose can be obtained in two different ways:\n - **Through the composition of poses**, but using $\\ominus p1$ instead of $p1$. Implement it in ``inverse_composition1()``.\n \n Mean: \n \n $$\n p12 = \\ominus p1 \\oplus p2 = f(\\ominus p1, p2) = \n \\begin{bmatrix} \n  x_{\\ominus p1} + x_{p2} cos \\theta_{\\ominus p1} - y_{p2} sin \\theta_{\\ominus p1} \\\\\n  y_{\\ominus p1} + x_{p2} sin \\theta_{\\ominus p1} + y_{p2} cos \\theta_{\\ominus p1} \\\\\n  \\theta_{\\ominus p1} + \\theta_{p2}\n \\end{bmatrix}\n $$\n \n Covariance:\n \n $$\n \\Sigma_{p12} = \\frac{\\partial p12}{\\partial \\ominus p1} \\frac{\\ominus p1}{\\partial p1} \\Sigma_{p1} \\frac{\\ominus p1}{\\partial p1}^T \\frac{\\partial p12}{\\partial \\ominus p1}^T \n +\n \\frac{\\partial p12}{\\partial p2} \\Sigma_{p2}  \\frac{\\partial p12}{\\partial p2}^T\n \\\\\n \\text{Applying the Chain rule} \\rightarrow \\Sigma_{p12} = \\frac{\\partial p12}{\\partial \\ominus p1} \\Sigma_{\\ominus p1} \\frac{\\partial p12}{\\partial \\ominus p1}^T\n +\n \\frac{\\partial p12}{\\partial p2} \\Sigma_{p2}  \\frac{\\partial p12}{\\partial p2}^T\n $$\n \n Being:\n \n $$\n \\frac{\\partial p12}{\\partial \\ominus p1} = \n \\begin{bmatrix}\n 1 & 0 & -x_{p2} sin \\theta_{\\ominus p1} - y_{p2} cos \\theta_{\\ominus p1} \\\\\n 0 & 1 & x_{p2} cos \\theta_{\\ominus p1} - y_{p2} sin \\theta_{\\ominus p1} \\\\\n 0 & 0 & 1\n \\end{bmatrix}  \n \\; \\; \\; \\; \\; \\frac{\\partial p12}{\\partial p2} = \n \\begin{bmatrix}\ncos \\theta_{\\ominus p1} & -sin \\theta_{\\ominus p1} & 0\\\\\nsin \\theta_{\\ominus p1} & cos \\theta_{\\ominus p1} & 0\\\\\n 0 & 0 & 1\n \\end{bmatrix}\n$$ $\\\\[5pt]$\n\n$$\n \\frac{\\partial \\ominus p1}{\\partial p1} = \n \\begin{bmatrix}\n -cos \\theta_{p1} & -sin \\theta_{p1} & x_{p1} sin \\theta_{p1} - y_{p1} cos \\theta_{p1} \\\\\n sin \\theta_{p1} & -cos \\theta_{p1} & x_{p1} cos \\theta_{p1} + y_{p1} sin \\theta_{p1}\\\\\n 0 & 0 & -1 \\\\\n \\end{bmatrix}\n \\; \\; \\; \\; \\; \\Sigma_{\\ominus p1} = \\frac{\\partial \\ominus p1}{\\partial p1} \\Sigma_{p1} \\frac{\\partial \\ominus p1}{\\partial p1}^T\n $$ $\\\\[5pt]$\n \n  - **Using the inverse composition of poses**, that is $p12 = \\ominus p1 \\oplus p2 = p2 \\ominus p1$. This one is given for you in ``inverse_composition2()``."},{"metadata":{"trusted":false},"cell_type":"code","source":"def inverse_composition1(p1_w, Qp1_w, p2_w, Qp2_w):\n    jac_inv_p = jac_tinv(p1_w)\n\n    inv_r1 = (\n        tinv(p1_w),\n        jac_inv_p @ Qp1_w @ jac_inv_p.T\n    )\n\n    jac_p12_inv = J1(inv_r1[0], p2_w)\n    jac_p12_p2 = J2(inv_r1[0], p2_w)\n\n    p12_w = tcomp(None, None)\n        \n    Qp12_w = (\n            None@inv_r1[1]@None \n            + None@Qp2_w@None\n        )\n    \n    return p12_w, Qp12_w","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def inverse_composition2(p1_w, Qp1_w, p2_w, Qp2_w):\n    dx, dy = p2_w[0, 0]-p1_w[0, 0], p2_w[1, 0]-p1_w[1, 0]\n    a = p2_w[2, 0] - p1_w[2, 0]\n    c, s = np.cos(p1_w[2, 0]), np.sin(p1_w[2, 0])\n\n    p12_w = np.array([\n        [dx*c + dy*s],\n        [-dx*s + dy*c],\n        [a]])\n    \n    jac_p12_r1 = np.array([\n        [-c, -s, -dx*s + dy*c],\n        [s, -c, -dx*c - dy*s],\n        [0, 0, -1]\n    ])\n\n    jac_p12_r2 = np.array([\n        [c, s, 0],\n        [-s, c, 0],\n        [0, 0, -1]\n    ])\n\n    #jac_p1_pinv = np.linalg.inv(jac_tinv(r1[0]))\n\n    Qp12_w = jac_p12_r1@Qp1_w@jac_p12_r1.T + jac_p12_r2@Qp2_w@jac_p12_r2.T\n\n    return p12_w, Qp12_w","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Robot R1\np1_w = np.vstack([1., 2., 0.5])\nQp1_w = np.diag([0.08, 0.6, 0.02])\n\n# Robot R2\np2_w = np.vstack([6., 4., 2.1])\nQp2_w = np.diag([0.20, 0.09, 0.03])\n\n# Obtain the relative pose p12 between both robots through the composition of poses\np12_w, Qp12_w = inverse_composition1(p1_w, Qp1_w, p2_w, Qp2_w)\nprint( '----\\tExercise 4.1.3 with method 1\\t----\\n'+\n        'p12_w = {}\\'\\n'.format(p12_w.flatten())+\n        'Qp12_w = \\n{}\\n'.format(Qp12_w))\n\n# Obtain the relative pose p12 between both robots through the inverse composition of poses\np12_w, Qp12_w = inverse_composition2(p1_w, Qp1_w, p2_w, Qp2_w)\nprint( '----\\tExercise 4.1.3 with method 2\\t----\\n'+\n        'p12_w = {}\\'\\n'.format(p12_w.flatten())+\n        'Qp12_w = \\n{}\\n'.format(Qp12_w))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<span style=\"color:blue\">Expected results:</span>\n ```\n p12_w = [ 5.34676389 -0.64196257  1.6       ]'\n \n Qp12_w = \n [[0.38248035 0.24115    0.01283925]\n [0.24115    1.16751965 0.10693528]\n [0.01283925 0.10693528 0.05      ]]\n ```"},{"metadata":{},"cell_type":"markdown","source":"### **<span style=\"color:green\"><b><i>ASSIGNMENT 4: Predicting an observation from the second robot</i></b></span>** \n\nAccording to the information (provided by R1) that we have about the position of the landmark $m$ in the world coordinates (its location $z_{1\\_w}$ and its associated uncertainty $W_{z_1\\_w}$), compute the *predicted observation* distribution of $z_{2p} =[r, \\alpha] \\sim N(z_{2p}, W_{2p})$ as taken by a range-bearing sensor mounted on `R2`. The image below shows this scenario.\n\n<center>\n    <img src=\"./images/assignment_4.png\"/>\n    <figcaption>Fig 6. Illustration of the scenario in assignment 4.</figcaption>\n</center>\n\n\nConsider the following:\n \n - The range-bearing model for taking measurements is *(Note: use [`np.arctan2()`](https://numpy.org/doc/stable/reference/generated/numpy.arctan2.html) for computing the angle. At this point, ignore the noise $w_i$)*:\n \n $$\n z_i = \\begin{bmatrix} r_i \\\\ \\alpha_i \\end{bmatrix} = h(x,m_i) + w_i = \n \\begin{bmatrix} \\sqrt((x_i-x)^2+(y_i-y)^2) \\\\ atan(\\frac{y_i-y}{x_i-x}) - \\theta \\end{bmatrix} \n + w_i\n $$ $\\\\[5pt]$\n \n - We need to compute the covariance of the predicted observation in Polar  coordinates $(W_{2p})$. For that, use the following: $\\\\[10pt]$\n \n    $$ W_{z2\\_c} = \\frac{\\partial f(p2,z_{1\\_w})}{\\partial \\ominus p2} \\frac{\\ominus p2}{\\partial p2} \\ Q_{p2\\_w} \\ \\frac{\\ominus p2}{\\partial p2}^T \\ \\left( \\frac{\\partial f(p2,z_{1\\_w})}{\\partial p} \\right)^T +\n        \\frac{\\partial f(p2,z_{1\\_w})}{\\partial z_{1\\_w}} \\ W_{z_1\\_w} \\ \\left( \\frac{\\partial f(p2,z_{1\\_w})}{\\partial z_{1\\_w}} \\right)^T\n   $$\n   \n    $$\n  \\text{Applying the Chain rule} \\rightarrow W_{z2\\_c} = \\frac{\\partial f(p2,z_{1\\_w})}{\\partial \\ominus p2} \\Sigma_{\\ominus p2} \\frac{\\partial f(p2,z_{1\\_w})}{\\partial \\ominus p2}^T\n +\n \\frac{\\partial f(p2,z_{1\\_w})}{\\partial p2} \\ W_{z_1\\_w} \\ \\frac{\\partial f(p2,z_{1\\_w})}{\\partial p2}^T\n $$\n  \nOnce you have the covariance expressed in cartesian coordinates, you can express it in polars by means of the following Jacobian:\n\n $$\n     \\frac{\\partial{p}}{\\partial{c}} = \n     \\begin{bmatrix}\n         \\cos{(\\alpha+\\theta)}  & \\sin{(\\alpha+\\theta)} \\\\\n         -\\sin{(\\alpha+\\theta)} / r  & \\cos{(\\alpha+\\theta)} / r\n     \\end{bmatrix}\n $$"},{"metadata":{"trusted":false},"cell_type":"code","source":"def predicted_obs_from_pov(p1_w, Qp1_w, z1_w, Wz1_w):\n    \"\"\" Method to translate a pose+covariance in the world frame to an observation.\n    \n        This method only translated the landmark to the pov of the robot.\n        It does not simulate a new observation.\n        \n        Args:\n            p1_w: Pose of the robot which acts as pov\n            Qp1_w: Covariance of the robot\n            z1_w: Landmark observed in cartesian coordinates(world frame)\n            Wz1_w: Covariance associated to the landmark.\n        Returns:\n            z2_pr: Expected observation of z1 from pov of p1_w\n            W2_p: Covariance associated to z2_pr\n    \"\"\"\n\n    # Take a measurement using the range-bearing model\n    z2_pr = np.vstack([\n            None, # distance\n            None # angle\n        ])\n    \n    # Obtain the uncertainty in the R2 reference frame using the composition of a pose and a landmark: \n    z1_ext = np.vstack([z1_w, 0]) # Prepare position and uncertainty shapes to the ones expected by inverse_composition\n    Wz1_w_ext = np.pad(Wz1_w, [(0, 1), (0, 1)], mode='constant')\n    _, Wz1_r = inverse_composition1(None, None, None, None)\n    W2_c = Wz1_r[0:2,0:2]\n      \n    # Jacobian from cartesian to polar at z2p_r\n    theta = z2_pr[1, 0] + p1_w[2, 0] \n    s, c = np.sin(theta), np.cos(theta)\n    r = z2_pr[0, 0]\n\n    Jac_car_pol = np.array([\n        [None, None],\n        [None, None]\n    ])\n\n    # Finally, propagate the uncertainty to polar coordinates in the\n    # robot frame\n    W2_p = None@None@None\n    \n    return z2_pr, W2_p","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"p2_w = np.vstack([6., 4., 2.1])\nQp2_w = np.diag([0.20, 0.09, 0.03])\n\nz2_pr, W2_p = predicted_obs_from_pov(p2_w, z1_w, Wz1)\nprint( '---- Exercise 4.1.4 ----\\n'+\n    'z2p_r = {}\\'\\n'.format(z2_pr.flatten())+\n    'W2_p = \\n{}\\n'.format(W2_p)    \n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<span style=\"color:blue\">Expected output:</span>\n```\n---- Exercise 4.1.4 ----\nz2p_r = [3.94880545 0.58862004]'\nW2_p = \n[[1.41886714 0.01057848]\n [0.01057848 0.07881227]]\n```"},{"metadata":{},"cell_type":"markdown","source":"### **<span style=\"color:green\"><b><i>ASSIGNMENT 5: Combining observations of the same landmark</i></b></span>** \n\nAssume now that a measurement $z_2 = [4 m., 0.3 rad.]^T$ of the landmark is taken from R2 with a sensor having the same precision as that of R1 ($W_{2p}= W_{1p}$). **You have to**:\n\n1. Use the previously implemented `to_world_frame()` function to compute the position and uncertainty about both measurements ($z1$ and $z2$) in the world frame.\n2. Plot the robots and the two measurements along with their uncertainty (ellipses) in the world frame.\n3. Combine both observations within the `combine_pdfs()` function, and show the resultant combined observation along with its associated uncertainty.\n    \n<figure style=\"text-align:center\">\n  <img src=\"images/fig4-1-2.png\" width=\"400\" alt=\"\">\n  <figcaption>Fig. 7: Results from the last exercise.</figcaption>\n</figure>    "},{"metadata":{"trusted":false},"cell_type":"code","source":"def combine_pdfs(z1_w, Wz1_w, z2_w, Wz2_w):\n    \"\"\" Method to combine the pdfs associated with two observations of the same landmark.  \n        \n        Args:            \n            z1_w: Landmark observed in cartesian coordinates(world frame) from Robot 1\n            Wz1_w: Covariance associated to the landmark.\n            z1_w: Landmark observed in cartesian coordinates(world frame) from Robot 2\n            Wz2_w: Covariance associated to the landmark.\n        Returns:\n            z: Combined observation\n            W_z: Uncertainty associated to z\n    \"\"\"\n    None # Implement the needed code here\n\n    W_z = None\n    z = None\n\n    return z, W_z\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"z2_p_r = np.vstack([4., .3])\nWz2_p_r = np.diag([0.25, 0.04])\n\nz1_w, Qz1 = to_world_frame(p1_w, Qp1_w, z1_p_r, W1)\nz2_w, Qz2 = to_world_frame(p2_w, Qp2_w, z2_p_r, W1)\n\n# Show results\nfig, ax = plt.subplots()\nplt.xlim([-.5, 10])\nplt.ylim([-.5, 10])\nplt.grid()\nplt.tight_layout()\n\nfig.canvas.draw()\n\nDrawRobot(fig, ax, None, label='R1', color='blue')\nPlotEllipse(fig, ax, p1_w, Qp1_w, color='blue')\n\nDrawRobot(fig, ax, None, label='R2', color='green')\nPlotEllipse(fig, ax, p2_w, Qp2_w, color='green')\n   \nax.plot(None, None, 'o', label='Z1', color='blue')\nPlotEllipse(fig, ax, z1_w, Qz1, color='blue')\n          \nax.plot(None, None, 'o', label='Z2', color='green')\nPlotEllipse(fig, ax, z2_w, Qz2, color='green')\n\nz_w, Wz_w = combine_pdfs(z1_w, Qz1, z2_w, Qz2)\nax.plot(z_w[0, 0], z_w[1, 0], 'o', label='Z3', color='red')\nPlotEllipse(fig, ax, z_w, Wz_w, color='red')\n          \nplt.legend()\n\n# Print results\nprint( '----\\tExercise 4.1.5\\t----\\n'+\n    'z2_w = {}\\'\\n'.format(z2_w.flatten())+\n    'Qz2 = \\n{}\\n'.format(Qz2)\n    )\n\n# Print results\nprint( '----\\tExercise 4.1.5 part 2\\t----\\n'+\n    'z_w = {}\\'\\n'.format(z_w.flatten())+\n    'Wz_w = \\n{}\\n'.format(Wz_w)\n    )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<span style=\"color:blue\">Expected ouputs:</span>\n\n### Sensor measurement from R2\n\n```\nz2_w = [3.05042514 6.70185272]'\nQz2 = \n[[0.84693794 0.4333316 ]\n [0.4333316  0.81306206]]\n```\n\n### Combined information\n```\n---- Exercise 4.1.5 parte 2 ----\nz_w = [2.58757252 6.15534036]'\nWz_w = \n[[0.37966125 0.07773125]\n [0.07773125 0.36999739]]\n```"},{"metadata":{},"cell_type":"markdown","source":"### <font color=\"blue\"><b><i>Thinking about it (1)</i></b></font>\n\nHaving completed the code above, you will be able to **answer the following questions**:\n\n- When working with landmarks, why do we ignore the information regarding orientation?\n\n    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n\n- In the two first assignments we computed the covariance matrix of the observation $z_1$ captured by robot $R1$ in two different cases: when the $R1$ pose was perfectly known, and having some uncertainty about it. Which covariance matrix was bigger? Is it bigger than that of the robot? Why? \n\n    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n    \n- When predicting an observation of $m$ from the second robot $R2$, why did we need to use the Jacobian ${\\partial{p}}/{\\partial{c}}$?\n\n    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n\n- In the last assignment we got two different pdf’s associated to the same landmark. Is that a contradiction? How did you manage two combine these two *pieces of information*?      \n\n    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>"},{"metadata":{},"cell_type":"markdown","source":"## <span style=\"color:green\">OPTIONAL</span>\n\n<span style=\"color:green\">As commented, a number of sensors can be mounted on a mobile robot. In the robotic sensing lecture we discused some of the most popular ones. As an optional exercise, you can look for interesting information about any of them (or any one not listed below) and further describe it here to complete your knowledge.</span>"},{"metadata":{},"cell_type":"markdown","source":"- Beacons\n  - GPS\n- Range sensors\n  - Sonar\n  - Infrared\n  - Laser scanner\n- Cameras\n- RGB-D cameras"},{"metadata":{},"cell_type":"markdown","source":"<span style=\"color:green\">***END OF OPTIONAL PART***</span>"},{"metadata":{},"cell_type":"markdown","source":"## <span style=\"color:green\">OPTIONAL</span>\n\n<span style=\"color:green\">An alternative to *landmark observation models* are *scan observation* ones, which work with scan-based sensors. Below, the three most popular ones are listed. Surf the internet for some code illustrating any of them, and include it in the notebook with a brief description of how it works and its purpose. You could also implement an example using these models.</span>"},{"metadata":{},"cell_type":"markdown","source":"## Scan observation models\n\nScan observation models are used when the sensor mounted on the robot provides a scan measuring distance and angle to obstacles in the workspace, *e.g.* a laser range finder. In this case, each element in the map is a cell described by its position (and probably a color representing if its free of obstacles or occupied), and data association is not explicitly addressed. \n\n#### Beam model\n#### Likelihood field\n#### Scan matching"},{"metadata":{},"cell_type":"markdown","source":"<span style=\"color:green\">***END OF OPTIONAL PART***</span>"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":2}