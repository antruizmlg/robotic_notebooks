{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 5.1 Least Squares Global Localization\n\nLeast Squares Positioning is a well-known algorithm for estimating the robot localization $x$ given a set of known landmarks in a map. Least Squares is akin to find the best pose $\\hat{x}$ by solving a system of equations of the form:\n\n$$z_{m \\times 1} = H_{m \\times n} \\cdot x_{n \\times 1}$$\n\nwhere: \n- $n$ is the length of the pose ($n=3$ in our case, position plus orientation),\n- $m$ represents the number of observations, and \n- $H$ is the matrix that codifies the observation model relating the measurement $z$ with the robot pose $x$. \n\nThis simple concept, nevertheless, has to be modified in order to be used  in real scenarios:"},{"metadata":{},"cell_type":"markdown","source":"## 5.1.1 Pseudo-inverse\n\nGenerally, to solve an equation system, we only need as many equations as variables. In the robot localization problem, each observation $z$ sets an equation, while the variables are the components of the state/pose, $x$. \n\nIn such a case, where $n=m$, a direct attempt to this problem exists:\n\n  $$x = H ^{-1} z$$\n\nSo a unique solution exists if $H$ is invertible, that is, $H$ is a square matrix with $det(H) \\neq 0$. \n\nHowever, in real scenarios typically there are available more observations than variables. An approach to address this could be to drop some of the additional equations, but given that observations $z$ are inaccurate (they have been affected by some noise), we may use the additional information to try to mitigate such noise. However, by doing that $H$ is no a squared matrix anymore, hence not being invertible. \n\nTwo tools can help us at this point. The first one is the utilization of **Least Squares** to find the closest possible $\\hat{x}$, i.e. the one where the the error ($e = Hx -z$) is minimal:\n\n  $$ \\hat x = \\arg\\min_{x} e^Te = [(z-Hx)^T(z-Hx)] = \\arg\\min_x || z-Hx ||^2$$\n\nwhich has a close form solution using the **pseudo-inverse** of a matrix:\n  \n  $$\\hat{x} = \\underbrace{(H^T H)^{-1} H^T}_{\\textit{pseudo-inverse }(H^+)} z$$\n\nThe **pseudo-inverse**, in contrast to the normal inverse operation, can be used in non-square matrices!  "},{"metadata":{"trusted":false},"cell_type":"code","source":"#%matplotlib notebook\n#%matplotlib inline\n\n# IMPORTS\n\nimport math\n\nimport numpy as np\nfrom numpy import linalg\nimport matplotlib\nmatplotlib.use('TkAgg')\nimport matplotlib.pyplot as plt\nimport scipy\nfrom scipy import stats\n\nimport sys\nsys.path.append(\"..\")\nfrom utils.PlotEllipse import PlotEllipse\nfrom utils.DrawRobot import DrawRobot\nfrom utils.tcomp import tcomp\nfrom utils.tinv import tinv, jac_tinv1 as jac_tinv\nfrom utils.Jacobians import J1, J2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **<span style=\"color:green\"><b><i>ASSIGNMENT 1: Playing with a robot in a corridor</i></b></span>** \n\nThe following code illustrates a simple scenario where a robot is in a corridor looking at a door, which is placed at the origin of the reference system (see Fig.1). The robot is equipped with a laser scanner able to measure distances, and takes a number of observations $z$. The robot is placed 3 meters away from the door, but this information is unknown for it. **Your goal is** to estimate the position of the robot in this 1D world using such measurements. $\\\\[5pt]$\n\n<figure style=\"text-align:center\">\n  <img src=\"images/corridor.png\" alt=\"\">\n  <figcaption>Fig. 1: Simple 1D scenario with a robot equipped with a laser scanner measuring distance to a door.</figcaption>\n</figure>\n\nThe following code cell shows the dimensions of all the actors involved in LS-positioning. Complete it for computing the robot pose $x$ from the available information. *Recall [`np.linalg.inv()`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.inv.html).* "},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"# Set the robot pose to unknown\nx = np.vstack(np.array([None]))\n\n# Sensor measurements to the door\nz = np.vstack(np.array([3.7,2.9,3.6,2.5,3.5]))\n\n# Observation model\nH = np.ones(np.array([5,1]))\n\nprint (\"Dimensions:\")\nprint (\"Pose x:         \" + str(x.shape))\nprint (\"Observations z: \" + str(z.shape))\nprint (\"Obs. model H:   \" + str(H.shape))\nprint (\"H.T@H:          \" + str((H.T@H).shape))\nprint (\"inv(H.T@H):     \" + str((np.linalg.inv(H.T@H)).shape))\nprint (\"H.T@z :         \" + str((H.T@z).shape))\n\n# Do Least Squares Positioning\nx = np.linalg.inv(H.T@H)@H.T@z\n\nprint('\\nLS-Positioning')\nprint('x = ' + str(x[0]))","execution_count":2,"outputs":[{"output_type":"stream","text":"Dimensions:\nPose x:         (1, 1)\nObservations z: (5, 1)\nObs. model H:   (5, 1)\nH.T@H:          (1, 1)\ninv(H.T@H):     (1, 1)\nH.T@z :         (1, 1)\n\nLS-Positioning\nx = [3.24]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"<span style=\"color:blue\">Expected output</span>\n\n```\nx = [3.24]\n```"},{"metadata":{},"cell_type":"markdown","source":"## 5.1.2 Weighted measurements\n\nIn cases where multiple sensors affected by different noise profiles are used, or in those where the robot is using a sensor with a varying error (*e.g.* typically radial laser scans are more accurate while measuring distances to close objects), it is interesting to weight the contribution of such measurements while retrieving the robot pose. For example, we are going to consider a sensor whose accuracy drops the further the observed landmark is. Given a *covariance* matrix $Q$ describing the error in the measurements, the equations above are rewritten as:\n\n  $$ \\hat x = \\arg\\min_{x} e^T Q^{-1} e = [(Hx-z)^TQ^{-1}(Hx-z)] $$\n\n  $$\n    \\begin{aligned}\n      &\\hat{x} \\leftarrow (H^T Q^{-1} H)^{-1} H^T Q^{-1} z &\\text{(1. Best estimation)}\\\\ \n      &\\Sigma_{\\hat{x}} \\leftarrow (H^T Q^{-1} H)^{-1} &\\text{(2. Uncertainty of the estimation)}\\\\\n    \\end{aligned}\n  $$\n  \n  Example with three measurements having different uncertainty ($\\sigma_1^2$, $\\sigma_2^2$, $\\sigma_3^2$):\n  \n  $$\n  e^T Q^{-1} e = [e_1 \\; e_2 \\; e_3]\n  \\begin{bmatrix} 1 / \\sigma_1^2 & 0 & 0 \\\\ 0 & 1/\\sigma_2^2 & 0 \\\\ 0 & 0 & 1/\\sigma_3^2 \\end{bmatrix} \n  \\begin{bmatrix} e_1 \\\\ e_2 \\\\ e_3 \\end{bmatrix}\n  = \\frac{e_1^2}{\\sigma_1^2} + \\frac{e_2^2}{\\sigma_2^2} + \\frac{e_3^2}{\\sigma_3^2}\n  = \\sum_{i=1}^m \\frac{e_i^2}{\\sigma_i^2}\n  $$\n  \n  In this way, the bigger the $\\sigma_i^2$, the smaller its contribution to the pose's computation.\n  \n  "},{"metadata":{},"cell_type":"markdown","source":"### **<span style=\"color:green\"><b><i>ASSIGNMENT 2: Adding growing uncertainty</i></b></span>** \n\nWe have new information! The manufacturer of the laser scanner mounted on the robot wrote an email telling us that the device is considerably more inaccurate for further distances. Concretely, such uncertainty is characterized by $\\sigma^2=e^z$ (the laser is not so accurate, being polite).\n\nWith this new information, implement the computation of the weighted LS-positioning so you can compare the previously estimated position with the new one."},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"# Sensor measurements to the door\nz = np.vstack(np.array([3.7,2.9,3.6,2.5,3.5]))\n\n# Uncertainty of the measurements\nQ = np.eye(5)*np.exp(z)\n\n# Observation model\nH = np.ones(np.array([5,1]))\n\n# Do Least Squares Positioning\nx = np.linalg.inv(H.T@H)@H.T@z\n\n# Do Weighted Least Squares Positioning\nx_w = np.linalg.inv((H.T@np.linalg.inv(Q)@H))@H.T@np.linalg.inv(Q)@z\n\nprint('\\nLS-Positioning')\nprint('x = ' + str(x[0]))\n\nprint('\\nWeighted-LS-Positioning')\nprint('x = ' + str(np.round(x_w[0],2)))","execution_count":3,"outputs":[{"output_type":"stream","text":"\nLS-Positioning\nx = [3.24]\n\nWeighted-LS-Positioning\nx = [3.01]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"<span style=\"color:blue\">Expected output</span>\n\n```\nLS-Positioning\nx = [3.24]\n\nWeighted-LS-Positioning\nx = [3.01]\n```"},{"metadata":{},"cell_type":"markdown","source":"## 5.1.3 Non-linear Least Squares\n\nUntil now we have assumed that $\\hat{x}$ can be solved as a simple system of equations, i.e. $H$ is a matrix. Nevertheless, typically observation models are non-linear, that is: $z = h(x)$, so the problem now becomes:\n\n  $$\n  \\hat x = \\arg\\min_x ||z-h(x)||^2\n  $$\n  \n  No close-form solutions exists for this new problem, but we can approximate it iteratively:\n  \n  $$ \n  \\textit{(Recall) Taylor expansion: } h(x) = h(x_0+\\delta) = h(x_0) + J_{h_0}\\delta \\\\\n  ||z-h(x)||^2 \\cong ||\\underbrace{z-h(x_0)}_{\\textit{error vector $e$}}-J_{h_o}\\delta||^2 = ||e-J_{h_0}\\delta||^2 \\leftarrow \\textit{$\\delta$ is unknown, $J_e=-J_{h_0}$}$$\n  \nSo we can define the equivalent optimization problem:\n  \n$$\n  \\delta = \\arg\\min_\\delta ||e + J_e \\delta||^2 \\rightarrow \\underbrace{\\delta}_{nx1} = -\\underbrace{(J_e^T J_e)^{-1}}_{nxn}\\underbrace{J_e^T}_{nxm} \\underbrace{e}_{mx1} \\textit{ ($\\delta$ that makes the previous squared norm minimum)}\n  $$\n  \n  The weighted form of the $\\delta$ computation results:\n  \n  $$\n  \\delta = (J_e^T Q^{-1} J_e)^{-1}J_e^T Q^{-1} e\n  $$\n  \nWhere:\n  \n  - $Q$ is the measurement covariance (*weighted measurement*)\n  - $J_e$ is the negative of the Jacobian of the observation model at $\\hat{x}$, also known as $\\nabla h_{\\hat{x}}$\n  - $e$ is the error of $z$ against $h(\\hat{x})$ (computed using the map information).\n  \nAs commented, there is no closed-form solution for the problem, but we can iteratively approximate it using the **Gauss-Newton algorithm**:\n\n  $$\n  \\begin{aligned}\n      &\\hat{x} \\leftarrow (\\dots)  &\\text{(1. Initial guess)} \\\\\n      &\\delta \\leftarrow (J_e^T Q^{-1} J_e)^{-1} J_e^T  Q^{-1} e  &\\text{(2. Evaluate delta/increment)} \\\\\n      &\\hat{x} \\leftarrow \\hat{x} - \\delta &\\text{(3. Update estimation)} \\\\\n      &\\textit{if }\\delta > \\textit{tolerance} \\rightarrow \\textit{goto (1.)} \\\\\n      &\\textit{else } \\rightarrow \\textit{return }\\hat{x} &\\text{(4. Exit condition)}\\\\\n  \\end{aligned}\n  $$\n  \n\n  "},{"metadata":{},"cell_type":"markdown","source":"### LS positioning in practice\n\nSuppose that a mobile robot equipped with a range sensor aims to localize itself in a map consisting of a number of landmarks by means of Least Squares and Gauss-Newton optimization.\n\nFor that, **you are provided with** the class `Robot` that implements the behavior of a robot that thinks that is placed at `pose` (that's its initial guess, obtained by composing odometry commands), but that has a real position `true pose`. In addition, the variable `cov` models the uncertainty of its movement, and `var_d` represents the variance (noise) of the range measurements. Take a look at it below.\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"class Robot(object):\n    \"\"\" Simulate a robot base and positioning.\n    \n        Attrs:\n            pose: Position given by odometry (in this case true_pose affected by noise)\n            true_pose: True position, selected by the mouse in this demo\n            cov: Covariance for the odometry sensor. Used to add noise to pose\n            var_d: Covariance (noise) of each range measurement\n    \"\"\"\n    def __init__(self,\n                 pose: np.ndarray,\n                 cov: np.ndarray,\n                 desv_d: int = 0):\n        # Pose related        \n        self.true_pose = pose\n        self.pose = pose + np.sqrt(cov)@scipy.randn(3, 1)\n        self.cov = cov\n        \n        # Sensor related\n        self.var_d = desv_d**2\n    \n    def plot(self, fig, ax, **kwargs):\n        DrawRobot(fig, ax, self.pose, color='red', label=\"Pose estimation (Odometry)\", **kwargs)\n        DrawRobot(fig, ax, self.true_pose, color=\"blue\", label=\"Real pose\", **kwargs)","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **<span style=\"color:green\"><b><i>ASSIGNMENT 3a: Computing distances from the robot to the landmarks</i></b></span>** \n\n**Implement the following function** to simulate how our robot observes the world. In this case, the landmarks in the map act as beacons: the robot can sense how far away they are without any information about angles. The robot uses a range sensor with the following observation model:\n\n  $$\n  z_i=[d_i]=h(m_i,x)=\\left[\\sqrt{(x_i-x)^2+(y_i-y)^2} \\; \\right]+w_i\n  $$ \n\n  where $m_i$ stands for the $i^{th}$ landmark, and $w_i$ is a noise added by the sensor.\n\n  Consider two scenarios in the function implementation:\n  - The measurment is carried out with an ideal sensor, so no noise nor uncertainty exists (`cov_d = 0`).\n  - The measurement comes from a real sensor affected by a given noise (`cov_d != 0`). We are going to consider that the range sensor is more accurate measuring distances to close landmarks than to far away ones. To implement this, consider that the noise grows with the root of the distance to the landmark, so the resultant uncertainty can be retrieved by: $\\\\[5pt]$\n$$\n \\sigma_{\\text{dist}} = \\sigma\\sqrt{z}\\\\[5pt]\n$$ \nthat is, `np.sqrt(z)*np.sqrt(cov_d)`. Recall that the sensor noise is modeled as a gaussian distribution, so you have to define such distribution and take samples from it  using the [`stats.norm()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html) and [`rvs()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.rvs.html) functions."},{"metadata":{"trusted":false},"cell_type":"code","source":"def distance(pose: np.ndarray, m: np.ndarray, cov_d: int = 0) -> np.ndarray:\n    \"\"\" Get observations for every landmark in the map.\n\n    In this case our observations are range only.\n    If cov_d > 0 then add gaussian noise with var_d covariance\n\n    Args:\n        pose: pose (true or noisy) of the robot taking observation\n        m: Map containing all landmarks\n        cov_d: Covariance of the sensor\n\n    Returns\n        z: numpy array containing distances to all obs. It has shape (nLandmars, 1). \n    \"\"\"\n    z = np.sqrt((m[0]-pose[0])**2 + (m[1]-pose[1])**2)\n    # compute distances to all landmarks\n        \n\n    if cov_d > 0:\n        z += np.sqrt(z)*np.sqrt(cov_d) # add noise if needed\n\n    return z","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Try your brand new function** with the following code:"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Define the robot pose, a map composed of 3 landmarks, and the sensor variance (we are using an ideal sensor)\npose = np.vstack([2, 2, 0.35])\nm = np.array([[-5,-15],[20,56],[54,-18]]).T\ncov_d = 0\n\n# Compute distances from the sensor to the landmarks\nz = distance(pose,m,cov_d)\n\n# Now consider a noisy sensor\ncov_d = 0.5\nnp.random.seed(seed=0)\nz_with_noise = distance(pose,m,cov_d)\n\n# Show the results\nprint('Measurements without noise:' + str(z))\nprint('Measurements with noise:   ' + str(z_with_noise))","execution_count":6,"outputs":[{"output_type":"stream","text":"Measurements without noise:[18.38477631 56.92099788 55.71355311]\nMeasurements with noise:   [21.41667145 62.25583611 60.99150504]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"<span style=\"color:blue\">Expected output</span>\n\n```\nMeasurements without noise:[18.38477631 56.92099788 55.71355311]\nMeasurements with noise:   [23.73319805 59.05577186 60.87928514]\n```"},{"metadata":{},"cell_type":"markdown","source":"### **<span style=\"color:green\"><b><i>ASSIGNMENT 3b: Implementing the algorithm</i></b></span>** \n\nFinally, we get to implement the Least Squares algorithm for localization. We ask you to complete the gaps in the following function, which:\n  - Starts by initializing the Jacobian of the observation function (`Jh`) and takes as initial guess (`xEst`) the position at which the robot thinks it is as given by its odometry (`R1.pose`).\n  - Then, it enters into a loop until convergence is reached, where:\n    1. The distances `zEst` to each landmark from the estimated position `xEst`are computed. Recall that the map (landmarks positions) are known (`w_map`).\n    - The error is computed by substracting to the obsevations provided by the sensor `z` the distances `zEst` computed at the previous point. Then, the residual error is computed as $e_{residual}=\\sqrt{e_x^2+e_y^2}$.\n    - The Jacobian of the observation model is evaluated at the estimated robot pose (`xEst`). This Jacobian has two columns and as many rows as observations to the landmarks: $\\\\[10pt]$\n    $$\n    jH = \n    \\begin{bmatrix} \n        \\frac{-1}{d_1}(x_1-x) & \\frac{-1}{d_1}(y_1-y) \\\\\n        \\frac{-1}{d_2}(x_2-x) & \\frac{-1}{d_2}(y_2-y) \\\\\n        \\cdots & \\cdots \\\\\n        \\frac{-1}{d_n}(x_n-x) & \\frac{-1}{d_n}(y_n-y) \\\\ \n    \\end{bmatrix}\n    $$    \nbeing $xEst=[x,y]$, $[x_i,y_i]$ the position of the $i^{th}$ landmark in the map, and $d$ the distance previously computed from the robot estimated pose $xEst$ to the landmarks. The jacobian of the error `jE`is just `-jH`.\n    - Computes the increment $\\delta$ (`incr`) and substract it to the estimated pose (`xEst`). *Note: recall that $\\delta = (J_e^T Q^{-1} J_e)^{-1}J_e^T Q^{-1} e$*"},{"metadata":{"trusted":false},"cell_type":"code","source":"def LeastSquaresLocalization(R1: Robot,\n                             w_map: np.ndarray,\n                             z: np.ndarray,\n                             nIterations=10,\n                             tolerance=0.001,\n                             delay=0.5) -> np.ndarray:\n    \"\"\" Pose estimation using Gauss-Newton for least squares optimization\n    \n        Args:\n            R1: Robot which pose we must estimate\n            w_map: Map of the environment\n            z: Observation received from sensor\n            \n            nIterations: sets the maximum number of iterations (default 10)\n            tolerance: Minimum error difference needed for stopping the loop (convergence) (default 0.001)\n            \n            delay: Wait time used to visualize the different iterations (default 0.5)\n            \n        Returns:\n            xEst: Estimated pose\n    \n    \"\"\"\n    \n    iteration = 0\n    \n    # Initialization of useful variables\n    incr = np.ones((2, 1)) # Delta\n    jH = np.zeros((w_map.shape[1], 2)) # Jacobian of the observation function of all the landmarks\n    xEst = R1.pose #Initial estimation is the odometry position (usually noisy)\n    \n    # Let's go!\n    while linalg.norm(incr) > tolerance and iteration < nIterations:\n        #if plotting:\n        plt.plot(xEst[0], xEst[1], '+r', markersize=1+math.floor((iteration*15)/nIterations))\n        # Compute the predicted observation (from xEst) and their respective Jacobians\n\n        # 1) TODO: Compute distance to each landmark from xEst (estimated observations w/o noise)\n        # \n        zEst = distance(xEst, w_map, cov_d=0)\n        \n        # 2) TODO: error = difference between real observations and predticed ones.\n        e = z - zEst\n        residual = np.sqrt(e.T@e) #residual error = sqrt(x^2+y^2)\n\n        # 3) TODO: Compute Jacobians with respect (x,y) (slide 13)\n        # The jH is evaluated at our current guest (xEst) -> z_p\n        \n        for i in range(0, zEst.size):\n            for j in range(0, 2):\n                jH[i, j] = (-1/zEst[i])*(w_map[j, i] - xEst[j, 0])\n        \n        jE = -jH\n\n        # The observation variances Q grow with the root of the distance\n        Q = np.diag(R1.var_d*np.sqrt(z))\n\n        # 4) TODO: Solve the equation --> compute incr\n        incr = (np.linalg.inv(jE.T@(np.linalg.inv(Q))@jE))@jE.T@(np.linalg.inv(Q))@e\n        \n        plt.plot([xEst[0, 0], xEst[0, 0]-incr[0]], [xEst[1, 0], xEst[1, 0]-incr[1]], 'r')\n        xEst[0:2, 0] -= incr\n\n        print (\"Iteration :\" + str(iteration))\n        print (\"  delta :   \" + str(incr))\n        print (\"  residual: \" + str(residual))\n        \n        iteration += 1\n        \n        plt.pause(delay)\n\n    plt.plot(xEst[0, 0], xEst[1, 0], '*g', markersize=14, label=\"Final estimation\") #The last estimation is plot in green\n    \n    return xEst","execution_count":87,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The next cell code launches our algorithm, so **we can try it!**. This is done according to the following steps:\n\n  1. The map `w_map` is built. In this case, the map consists of a number of landmarks (`nLandmarks`).\n  2. The program asks the user to set the true position of the robot (`xTrue`) by clicking with the mouse in the map.\n  3. A new pose is generated from it, `xOdom`, which represents the pose that the robot thinks it is in. This simulates a motion command from an arbitrary pose that ends up with the robot in `xTrue`, but it thinks that it is in `xOdom`.\n  4. Then the robot takes a (noisy) range measurement to each landmark in the map.\n  5. Finally, the robot employs a Least Squares definition of the problem and Gauss-Newton to iteratively optimize such a guess (`xOdom`), obtaining a new (and hopefully better) estimation of its pose `xEst`.\n\n  **Example**\n\n  The figure below shows an example of execution of this code (once completed).\n\n<figure style=\"text-align:center\">\n  <img src=\"images/fig5-1-1.png\" width=\"500\" alt=\"\">\n  <!--figcaption>Fig. 1: </figcaption-->\n</figure>"},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"def main(nLandmarks=7, env_size=140):\n    # MATPLOTLIB\n    fig, ax = plt.subplots() \n    plt.xlim([-90, 90])\n    plt.ylim([-90, 90])\n    plt.grid()\n    plt.ion()\n    plt.tight_layout()\n\n    fig.canvas.draw()\n    \n    # VARIABLES\n    num_landmarks = 7 # number of landmarks in the environment\n    env_size = 140 # A square environment with x=[-env_size/2,env_size/2] and y=[-env_size/2,env_size/2]\n       \n    # MAP CREATION AND VISUALIZATION\n    w_map = env_size*scipy.rand(2, num_landmarks) - env_size/2 # randomly place the landmarks in the map\n    ax.plot(w_map[0, :], w_map[1, :], 'o', color='magenta', label=\"Landmarks\")\n    \n    # ROBOT POSE AND SENSOR INITIALIZATION \n    desv_d = 0.5 # standard deviation (noise) of the range measurement\n    cov = np.diag([25, 30, np.pi*180])**2 # covariance of the motion (odometry)\n    xStart = np.vstack(plt.ginput(1)).T # get the robot starting point from the user\n    robot_pose=np.vstack([xStart, 0]) # robot_pose\n    \n    R1 = Robot(robot_pose, cov, desv_d)\n    R1.plot(fig, ax)\n\n    # MAIN\n    z = distance(R1.true_pose, w_map, cov_d=R1.var_d) # take (noisy) measurements to the landmarks\n    LeastSquaresLocalization(R1, w_map, z) # LS Positioning!\n    \n    # PLOTTING RESULTS\n    plt.legend()\n    fig.canvas.draw()\n\n# RUN    \nmain()","execution_count":89,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"need at least one array to concatenate","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-89-fc698f2b7b9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# RUN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-89-fc698f2b7b9b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(nLandmarks, env_size)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mdesv_d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;31m# standard deviation (noise) of the range measurement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mcov\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m180\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;31m# covariance of the motion (odometry)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mxStart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mginput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;31m# get the robot starting point from the user\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mrobot_pose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mxStart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# robot_pose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/srv/conda/envs/notebook/lib/python3.7/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \"\"\"\n\u001b[1;32m    282\u001b[0m     \u001b[0m_warn_for_nonsequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"]},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"### <font color=\"blue\"><b><i>Thinking about it (1)</i></b></font>\n\nHaving completed this notebook above, you will be able to **answer the following questions**:\n\n- What are the dimensions of the error residuals? Does them depend on the number of observations? \n\n    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Las dimensiones son 1x1, lo que obtenemos es un escalar. No porque se obtiene de multiplicar la matriz de errores traspuesta por la matriz de errores, como la matriz de errores tiene mx1 componentes, siendo m el número de observaciones, entonces lo que hacemos es multiplicar una matriz de dimensiones 1xm por una matriz de dimensión mx1, como resultado obtenemos un escalar. </i></p>\n\n- Why is Weighted LS obtaining better results than LS?\n\n    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Porque le estamos dando a cada observación un peso, de manera que las observaciones que tengan mayor probabiliad de error (en este caso las que se hacen a una mayor distancia) son las que tienen menos peso a la hora de calcular la pose del robot, y las observaciones más fiables son las que cobran mayor importancia.</i></p>\n\n- Which is the minimum number of landmarks needed for localizing the robot? Why?\n\n    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Necesitamos tantos landmarks como componentes tenga la pose del robot porque de esa manera la ecuación tiene una solución ya que tenemos más observaciones (m) que incógnitas (n). Si m es menor que n existen infinitas soluciones para el sistema de ecuaciones y, por lo tanto, infinitas soluciones para la pose del robot.</i></p>\n\n- Play with different “qualities” of the range sensor. Could you find a value for its variance so the LS method fails?\n\n    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n\n- Play also with different values for the odometry uncertainty. What does this affect? \n\n    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":2}